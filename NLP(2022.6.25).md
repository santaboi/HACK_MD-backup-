# NLP (2022/6/25)
+ RNN : seq2seq (不好平行化)
+ CNN : seq2seq (可以平行化，要很多層才能看較長seq)

## Self Attention 
+ pros : 可以繞過bottleneck
+ pros : 檢查注意力分布，有好的 **"可解釋性"**
+ 用 self-attention 取代bidirectional RNN
    + self-attention (天涯若比鄰)
    + attention -> 吃兩個向量output一個分數
        + 實際操作 : query 跟 key 做內積再除以其維度(把維度的影響拿掉)
![](https://i.imgur.com/aBTiAkT.png)
`[求b1-b4中的b1] :1. q1 對所有key做 內積 2. softmax 完成得a-head 3. 乘上v1加權 做 weighted sum 得b2`以此類推

    + attention 為何善於平行化 (把vector concat 一起算)
    1-1.![](https://i.imgur.com/5rU4Tnm.png)
    1-2.![](https://i.imgur.com/ooLnWZR.png)
    2-1.(全部的平行)![](https://i.imgur.com/bNI58Er.png)

    2-2.(全部的平行)
    ![](https://i.imgur.com/VFJ6RMp.png)
    conclusion : 
    ![](https://i.imgur.com/sWniXgA.png)
## MultiHead Self-Attention
+ **多head 好處** : (**head1**) 可能看local 資訊 , (**head2**) 可能看global
    + head 數量其中一個**超參數**
    + 維度是獨立處理再concat 起來
    ![](https://i.imgur.com/bUwDdbr.png)
    ![](https://i.imgur.com/b2Ipkv0.png)
    + concat起來之後 可再乘transform 降維
    + 
## Self Attention 缺點跟解決方法
+ 上面的方法，缺少positional資訊
+ 原本的Xi 跟 Pi concat (Pi是第i維是1其餘都0)
    + pi 帶有位置資訊
    + 結果來看就是ai + ei
    ![](https://i.imgur.com/rnDT63r.png)
    + 小補充 (layer norm vs batch norm)
        + batch norm 是希望同batch不同data的相同維度 
        + layer norm 是希望同一筆資料之間的不同維度(通常搭配rnn)
       **達成 miu = 0 , sigma = 1**
        ![](https://i.imgur.com/F8xgntd.png)
+ self attention整體流程圖
    + masked 就是只看encoder已經輸出的部分而已，看不到future
![](https://i.imgur.com/XriB2aa.png)



